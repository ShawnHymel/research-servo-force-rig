{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6ce5ea-9e20-42d1-ad32-efd561273985",
   "metadata": {},
   "source": [
    "From: https://keras.io/guides/writing_a_custom_training_loop_in_torch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2807656d-a914-46eb-acd4-8669dd655288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7e3691-46e6-430c-99f6-a615398bb9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"KERAS_BACKEND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dbf18b1-62ac-4381-b5ba-bb9f2792dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default devcice\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef33c8fb-d627-4bd0-99bb-9bcd799c2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider a simple MNIST model\n",
    "def get_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312be2be-83f8-41ac-8440-62ba6caef8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Create load up the MNIST dataset and put it in a torch DataLoader\n",
    "# Prepare the training dataset.\n",
    "batch_size = 32\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d1e541-df97-4298-9152-38f09aafe6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eab4cdfc-f389-4b7e-b429-aaeb9d7a5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch Datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_train).float().to(device),\n",
    "    torch.from_numpy(y_train).float().to(device)\n",
    ")\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_val).float().to(device),\n",
    "    torch.from_numpy(y_val).float().to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94a2b737-ee84-4fa1-9a1c-c8163d47dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for the Datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "555809a2-3743-4b88-bbf2-b3d6101b1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a torch optimizer\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Instantiate a torch loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4518310c-0c44-4f57-98b3-aadd62055ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for 1 batch) at step 0: 0.0440\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 0.2010\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 0.1639\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 0.5320\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 0.4841\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.0258\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 0.0401\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 0.1123\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.2103\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 0.0310\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.1509\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.1254\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.3798\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.0109\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.1499\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.2474\n",
      "Seen so far: 48032 samples\n",
      "Training loss (for 1 batch) at step 0: 0.0775\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 0.2651\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 0.5946\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 0.1098\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 0.4346\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.4864\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 0.0397\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 0.2339\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.2050\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 0.0696\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.0747\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.0638\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.2565\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.1381\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.2069\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.5187\n",
      "Seen so far: 48032 samples\n",
      "Training loss (for 1 batch) at step 0: 0.0490\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 0.2547\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 0.0312\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 0.2204\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 0.1162\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.1664\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 0.0677\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 0.3059\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.0454\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 0.0100\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.0855\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.2168\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.2024\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.2933\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.0310\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.1284\n",
      "Seen so far: 48032 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Move inputs and targets to the same device as the model\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer variable updates\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().cpu().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5d75c48-b8cb-4916-8064-f94703940a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for 1 batch) at step 0: 226.0276\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 3.1136\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 4.1769\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 1.9400\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 1.6995\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.5839\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 1.0641\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 1.4072\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.2439\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 1.5329\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.2775\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.2846\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.3568\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.6290\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.7858\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.1519\n",
      "Seen so far: 48032 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for 1 batch) at step 0: 0.5018\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 0.2260\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 0.4713\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 0.2544\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 0.1100\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.5984\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 0.3858\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 0.4967\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.4440\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 0.4807\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.5617\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.2087\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.3550\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.4209\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.2543\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.1035\n",
      "Seen so far: 48032 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for 1 batch) at step 0: 0.2020\n",
      "Seen so far: 32 samples\n",
      "Training loss (for 1 batch) at step 100: 0.3563\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for 1 batch) at step 200: 0.5436\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for 1 batch) at step 300: 0.1478\n",
      "Seen so far: 9632 samples\n",
      "Training loss (for 1 batch) at step 400: 0.1190\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for 1 batch) at step 500: 0.0719\n",
      "Seen so far: 16032 samples\n",
      "Training loss (for 1 batch) at step 600: 0.0882\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for 1 batch) at step 700: 1.0377\n",
      "Seen so far: 22432 samples\n",
      "Training loss (for 1 batch) at step 800: 0.1598\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for 1 batch) at step 900: 0.0246\n",
      "Seen so far: 28832 samples\n",
      "Training loss (for 1 batch) at step 1000: 0.2831\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for 1 batch) at step 1100: 0.0994\n",
      "Seen so far: 35232 samples\n",
      "Training loss (for 1 batch) at step 1200: 0.1119\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for 1 batch) at step 1300: 0.1479\n",
      "Seen so far: 41632 samples\n",
      "Training loss (for 1 batch) at step 1400: 0.1542\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for 1 batch) at step 1500: 0.2730\n",
      "Seen so far: 48032 samples\n"
     ]
    }
   ],
   "source": [
    "# Keras version\n",
    "model = get_model()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(targets, logits)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        trainable_weights = [v for v in model.trainable_weights]\n",
    "\n",
    "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
    "        # for the weights.\n",
    "        loss.backward()\n",
    "        gradients = [v.value.grad for v in trainable_weights]\n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            optimizer.apply(gradients, trainable_weights)\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().cpu().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb41648-7958-455b-9b6e-189e44679456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
